# spectral_merging_b.py
# B ç‰ˆç®—æ³•ï¼šPruning as Alignmentï¼ˆåŸºäºŽå…¬å…±é”šç‚¹çš„æ•æ„Ÿåº¦å‰ªæžï¼‰

from __future__ import annotations

import copy
from typing import Dict, List, Optional

import torch
import torch.nn.functional as F
import swanlab
from peft import set_peft_model_state_dict, get_peft_model_state_dict
from transformers import CLIPTokenizer
import torch.nn.functional as F
class SensitivityAggregator:
    """
    åŸºäºŽæ¢¯åº¦æ•æ„Ÿåº¦çš„è¯­ä¹‰å‰ªæžèšåˆå™¨ (Pruning as Alignment)ã€‚

    è¿™é‡Œåªè´Ÿè´£â€œå¦‚ä½•ç”¨ç»™å®šçš„ model + anchor_dataloader åšå‰ªæžå’Œèšåˆâ€ï¼Œ
    ä¸è´Ÿè´£æž„å»º CLIP+LoRA æ¨¡åž‹æˆ–é”šç‚¹ DataLoaderï¼Œè¿™äº›åœ¨ server/strategy å±‚å®Œæˆã€‚
    """

    def __init__(
        self,
        model,
        anchor_dataloader,
        device: torch.device | str = "cuda",
        prune_ratio: float = 0.7,
        client_domains: Optional[List[str]] = None,
    ) -> None:
        """
        Args:
            model: åŸºåº§æ¨¡åž‹ (CLIP + LoRA ç»“æž„)ï¼Œéœ€ä¸Žå®¢æˆ·ç«¯è®­ç»ƒæ—¶ç»“æž„å®Œå…¨ä¸€è‡´ã€‚
            anchor_dataloader: å…¬å…±é”šç‚¹ DataLoaderï¼Œbatch å½¢å¦‚ï¼š
                               {"pixel_values": Tensor[B,3,H,W], "input_ids": Tensor[B,L]}
            device: è®¡ç®—è®¾å¤‡ã€‚
            prune_ratio: å‰ªæžçŽ‡ï¼Œ0.7 è¡¨ç¤ºè®¤ä¸º 70% çš„ä½Žæ•æ„Ÿåº¦å‚æ•°æ˜¯å™ªéŸ³ï¼Œä¼šè¢«è£æŽ‰ã€‚
            client_domains: å®¢æˆ·ç«¯åŸŸååˆ—è¡¨ï¼Œç”¨äºŽåœ¨ SwanLab é‡Œæ‰“ Server/Anchor_Loss_{Domain} ç­‰æŒ‡æ ‡ã€‚
        """
        self.model = model
        self.anchor_dataloader = anchor_dataloader
        self.device = device
        self.prune_ratio = prune_ratio
        self.client_domains: Optional[List[str]] = client_domains

        # aggregate è¢«è°ƒç”¨çš„è½®æ¬¡è®¡æ•°ï¼Œç”¨ä½œ SwanLab ä¸­çš„ round ç»´åº¦
        self.round_index: int = 0

    def compute_saliency_and_prune(self, client_state_dict: Dict[str, torch.Tensor], client_index: Optional[int] = None) -> Dict[str, torch.Tensor]:
        """
        [ç»ˆæžä¿®å¤ç‰ˆ] æ ¸å¿ƒæ–¹æ³•ï¼šå¯¹å•ä¸ªå®¢æˆ·ç«¯çš„ LoRA å‚æ•°è¿›è¡Œã€ä½“æ£€ -> å‰ªæž -> ç¼©æ”¾ã€‘
        
        åŠŸèƒ½æ¸…å•ï¼š
        1. âœ… ä½¿ç”¨å®˜æ–¹ API (set_peft_model_state_dict) è§£å†³ Key Mismatchã€‚
        2. âœ… å¢žåŠ  B çŸ©é˜µéžé›¶æ£€æŸ¥ï¼Œé˜²æ­¢åŠ è½½ç©ºå£³å‚æ•°ã€‚
        3. âœ… è‡ªåŠ¨åŠ è½½ Tokenizer (æ”¯æŒ HF é•œåƒ/æœ¬åœ°ç¼“å­˜)ã€‚
        4. âœ… é²æ£’çš„æ•°æ®è§£åŒ…ï¼šå…¼å®¹ PyTorch List/Tuple å’Œ HuggingFace Dictã€‚
        5. âœ… æ™ºèƒ½æ–‡æœ¬æž„é€ ï¼šä¼˜å…ˆç”¨çœŸå®žæ ‡ç­¾ (dataset.classes)ï¼Œå¤±è´¥åˆ™ç”¨ Dummy Promptã€‚
        """
        print(f"\n âœ… [Server] å¼€å§‹å¤„ç†å®¢æˆ·ç«¯ {client_index} çš„å‚æ•° (Saliency Pruning)...")
        
        # =======================================================
        # 1. åŠ è½½å‚æ•° (Loading with Official API)
        # =======================================================
        try:
            # å®˜æ–¹ API ä¼šè‡ªåŠ¨å¤„ç† base_model.model å‰ç¼€é—®é¢˜
            set_peft_model_state_dict(self.model, client_state_dict)
        except Exception as e:
            print(f"âŒ [åŠ è½½å¼‚å¸¸] set_peft_model_state_dict æŠ›å‡ºé”™è¯¯: {e}")
            raise e

        self.model.to(self.device)
        
        # =======================================================
        # ðŸ›¡ï¸ é˜²å¾¡å±‚: éªŒè¯ LoRA æ˜¯å¦çœŸçš„åŠ è½½è¿›åŽ»äº†ï¼Ÿ
        # =======================================================
        zero_b_count = 0
        total_b_count = 0
        for name, param in self.model.named_parameters():
            if "lora_B" in name:
                total_b_count += 1
                if torch.all(param.data == 0):
                    zero_b_count += 1
        
        if total_b_count > 0 and zero_b_count == total_b_count:
            raise RuntimeError("âŒ [è‡´å‘½é”™è¯¯] Server ç«¯ LoRA å‚æ•°åŠ è½½å¤±è´¥ï¼æ‰€æœ‰çš„ lora_B çŸ©é˜µéƒ½æ˜¯ 0ï¼")
        elif zero_b_count > 0:
            print(f"âš ï¸ [è­¦å‘Š] å‘çŽ° {zero_b_count}/{total_b_count} ä¸ª lora_B çŸ©é˜µä¾ç„¶ä¸º 0ã€‚")
        else:
            print(f"âœ… [æˆåŠŸ] LoRA å‚æ•°åŠ è½½éªŒè¯é€šè¿‡ (BçŸ©é˜µéžé›¶)ã€‚")

        # =======================================================
        # 2. å‡†å¤‡ Tokenizer & çœŸå®žæ ‡ç­¾æ˜ å°„
        # =======================================================
        tokenizer = None
        if CLIPTokenizer is not None:
            try:
                # åœ¨ç»ˆç«¯é…ç½® export HF_ENDPOINT=https://hf-mirror.com
                tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
            except Exception as e:
                print(f"âš ï¸ [è­¦å‘Š] Tokenizer åŠ è½½å¤±è´¥ ({e})ï¼Œå°†å°è¯•ä»…ä½¿ç”¨å›¾åƒç‰¹å¾æˆ–è·³è¿‡ã€‚")
        
        # å°è¯•ä»Ž DataLoader æå–çœŸå®žçš„ç±»åˆ«åç§° (ä¾‹å¦‚ ["Dog", "Cat", ...])
        real_class_names = None
        if hasattr(self.anchor_dataloader, 'dataset'):
            ds = self.anchor_dataloader.dataset
            if hasattr(ds, 'classes') and isinstance(ds.classes, (list, tuple)):
                real_class_names = ds.classes
                # print(f"[Server] å·²æå–çœŸå®žç±»åˆ«è¡¨ï¼Œå…± {len(real_class_names)} ç±»")

        # =======================================================
        # 3. å‡†å¤‡æ¢¯åº¦è®¡ç®—
        # =======================================================
        # å†»ç»“éž LoRA å‚æ•°ï¼Œå¼€å¯ LoRA æ¢¯åº¦
        for name, param in self.model.named_parameters():
            if "lora_" in name:
                param.requires_grad = True
            else:
                param.requires_grad = False
        
        self.model.zero_grad()
        
        # =======================================================
        # 4. å‰å‘ä¼ æ’­ä¸Žåå‘ä¼ æ’­ (Diagnosis)
        # =======================================================
        total_loss = 0.0
        batch_count = 0
        
        if len(self.anchor_dataloader) == 0:
             print("âš ï¸ [è­¦å‘Š] Anchor DataLoader ä¸ºç©ºï¼ç›´æŽ¥è¿”å›žåŽŸå‚æ•°ã€‚")
             return client_state_dict

        print(f"[Server] æ­£åœ¨ä½¿ç”¨å…¬å…±é”šç‚¹æ•°æ®è®¡ç®—æ¢¯åº¦æ•æ„Ÿåº¦...")
        
        for batch_idx, batch in enumerate(self.anchor_dataloader):
            images = None
            input_ids = None
            labels = None

            # --- [é²æ£’è§£åŒ…] å…¼å®¹ Dict å’Œ List/Tuple ---
            if isinstance(batch, dict):
                images = batch.get('pixel_values')
                if images is None:
                    images = batch.get('images')
                input_ids = batch.get('input_ids') # å¦‚æžœæ˜¯ HF å¤„ç†å¥½çš„æ•°æ®ï¼Œè¿™é‡Œä¼šæœ‰ input_ids
            elif isinstance(batch, (list, tuple)):
                images = batch[0]
                if len(batch) > 1:
                    # æ£€æŸ¥ç¬¬äºŒä¸ªå…ƒç´ æ˜¯ æ–‡æœ¬ è¿˜æ˜¯ æ•°å­—æ ‡ç­¾
                    second_element = batch[1]
                    if isinstance(second_element, torch.Tensor) and second_element.dtype in [torch.long, torch.int]:
                        labels = second_element # æ˜¯æ•°å­—æ ‡ç­¾
                    else:
                        input_ids = second_element # å¯èƒ½æ˜¯ input_ids æˆ–è€… æ–‡æœ¬åˆ—è¡¨
            else:
                continue # è·³è¿‡æœªçŸ¥æ ¼å¼

            if images is None:
                continue

            # --- [æ–‡æœ¬æž„é€ é€»è¾‘] ---
            # ä¼˜å…ˆçº§ 1: DataLoader ç›´æŽ¥æä¾›äº† input_ids -> ç›´æŽ¥ç”¨
            # ä¼˜å…ˆçº§ 2: æä¾›äº† input_ids æ–‡æœ¬åˆ—è¡¨ -> çŽ°åœº Tokenize
            # ä¼˜å…ˆçº§ 3: æä¾›äº†æ•°å­—æ ‡ç­¾ (labels) + æœ‰å¯¹ç…§è¡¨ (real_class_names) -> æŸ¥è¡¨é€ å¥ -> Tokenize
            # ä¼˜å…ˆçº§ 4: å•¥éƒ½æ²¡ -> é€ å‡å¥ (Dummy) -> Tokenize

            if input_ids is None and tokenizer is not None:
                texts_to_tokenize = []
                
                # å°è¯•ä½¿ç”¨çœŸå®žæ ‡ç­¾
                if labels is not None and real_class_names is not None:
                    class_indices = labels.tolist()
                    # æ˜ å°„å¹¶æ¸…ç†ä¸‹åˆ’çº¿ (Alarm_Clock -> Alarm Clock)
                    names = [real_class_names[i].replace("_", " ") if i < len(real_class_names) else "object" for i in class_indices]
                    texts_to_tokenize = [f"a photo of a {name}" for name in names]
                    print(f"texts_to_tokenize ä½¿ç”¨çœŸå®žæ ‡ç­¾: {texts_to_tokenize}")
                # å¦åˆ™ä½¿ç”¨å…œåº•æ–‡æœ¬
                else:
                    texts_to_tokenize = ["a photo of an object"] * images.size(0)
                    print(f"texts_to_tokenize ä½¿ç”¨å…œåº•æ–‡æœ¬: {texts_to_tokenize}")
                # æ‰§è¡Œ Tokenize
                try:
                    tokenized = tokenizer(texts_to_tokenize, padding=True, truncation=True, max_length=77, return_tensors="pt")
                    input_ids = tokenized["input_ids"]
                except Exception as e:
                    print(f"âŒ Tokenize å¤±è´¥: {e}")
                    continue

            # --- å†æ¬¡æ£€æŸ¥ input_ids ---
            if input_ids is None:
                print("âš ï¸ [è·³è¿‡] æ— æ³•æž„å»ºæ–‡æœ¬è¾“å…¥ï¼Œè·³è¿‡æ­¤ Batchã€‚")
                continue

            # ç§»åŠ¨åˆ° GPU
            images = images.to(self.device)
            input_ids = input_ids.to(self.device)
            
            # Forward
            # æ³¨æ„ï¼šCLIP éœ€è¦ image å’Œ text åŒæ—¶è¾“å…¥æ‰èƒ½è®¡ç®—å¯¹æ¯”æŸå¤±
            outputs = self.model(input_ids=input_ids, pixel_values=images)
            
            # Loss Calculation (Image-Text Matching)
            logits_per_image = outputs.logits_per_image
            logits_per_text = outputs.logits_per_text
            
            # æž„é€ å¯¹è§’çº¿ Ground Truth (å‡è®¾ Batch å†…æ˜¯ä¸€ä¸€å¯¹åº”çš„)
            current_bs = images.size(0)
            ground_truth = torch.arange(current_bs, device=self.device)
            
            loss = (F.cross_entropy(logits_per_image, ground_truth) + 
                    F.cross_entropy(logits_per_text, ground_truth)) / 2
            
            # Backward
            loss.backward()
            
            total_loss += loss.item()
            batch_count += 1
            
            # åªè¦è·‘ 5 ä¸ª Batch å°±å¤Ÿäº†
            if batch_count >= 5: 
                break
        
        avg_loss = total_loss / batch_count if batch_count > 0 else 0.0
        print(f"    > [Diagnosis] Anchor Loss: {avg_loss:.4f}")

        # =======================================================
        # 5. å‰ªæžä¸Žç¼©æ”¾ (Surgery)
        # =======================================================
        pruned_count = 0
        total_lora_params = 0
        # ç”¨äºŽç»Ÿè®¡æ•æ„Ÿåº¦çš„åˆ†å¸ƒæƒ…å†µ
        all_saliency_stats = []

        # ä¸´æ—¶å…³é—­æ¢¯åº¦è®°å½•ï¼Œè¿›è¡Œ In-Place ä¿®æ”¹
        with torch.no_grad():
            for name, param in self.model.named_parameters():
                if "lora_" in name:
                    total_lora_params += 1
                    
                    if param.grad is None:
                        # æ²¡æœ‰æ¢¯åº¦çš„å‚æ•°è§†ä¸ºåºŸå¼ƒï¼Œç½®é›¶
                        param.data.fill_(0.0) 
                        continue
                    
                    # è®¡ç®—æ•æ„Ÿåº¦
                    saliency = (param.data * param.grad).abs()
                    # --- [ç»Ÿè®¡] è®°å½•è¿™ä¸€å±‚çš„å¹³å‡æ•æ„Ÿåº¦ ---
                    layer_mean = saliency.mean().item()
                    layer_max = saliency.max().item()
                    all_saliency_stats.append(layer_mean)

                    num_params = saliency.numel()
                    
                    if num_params > 0:
                        # ç¡®å®šé˜ˆå€¼
                        k = int(num_params * self.prune_ratio)
                        if k > 0:
                            threshold = torch.kthvalue(saliency.view(-1), k).values
                            mask = (saliency >= threshold).float()
                        else:
                            mask = torch.ones_like(saliency)
                        
                        # ä¿å­˜åŽŸå§‹èƒ½é‡ç”¨äºŽç¼©æ”¾
                        original_data = param.data.clone()
                        
                        # æ‰§è¡Œå‰ªæž
                        param.data.mul_(mask)
                        # --- [ç»Ÿè®¡] è¿™ä¸€å±‚å®žé™…å‰ªäº†å¤šå°‘ ---
                        # mask é‡Œ 0 çš„ä¸ªæ•°å°±æ˜¯è¢«å‰ªæŽ‰çš„ä¸ªæ•°
                        layer_pruned = num_params - mask.sum().item()
                        pruned_count += int(layer_pruned)

                        
                        # èƒ½é‡è¡¥å¿ (Rescaling)
                        energy_original = original_data.abs().sum()
                        energy_pruned = param.data.abs().sum()
                        
                        if energy_pruned > 1e-6:
                            scale_factor = energy_original / energy_pruned
                            # é™åˆ¶ç¼©æ”¾å€æ•°ï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸
                            scale_factor = torch.clamp(scale_factor, max=10.0)
                            param.data.mul_(scale_factor)
                        
                        if k > 0: pruned_count += 1
        # =======================================================
        # ðŸ“Š [æ‰“å°] æ•æ„Ÿåº¦æŠ¥å‘Š
        # =======================================================
        global_avg_saliency = sum(all_saliency_stats) / len(all_saliency_stats) if all_saliency_stats else 0
        prune_percentage = (pruned_count / total_lora_params * 100) if total_lora_params > 0 else 0
        
        print(f"    > [Report] æ•æ„Ÿåº¦ç»Ÿè®¡:")
        print(f"      - LoRA å‚æ•°æ€»é‡: {total_lora_params}")
        print(f"      - å¹³å‡æ•æ„Ÿåº¦ (Mean Saliency): {global_avg_saliency:.6f} (å¦‚æžœä¸ä¸º0ï¼Œè¯´æ˜Žè®¡ç®—æˆåŠŸ)")
        print(f"      - å®žé™…å‰ªæžæ•°é‡: {pruned_count} ({prune_percentage:.2f}%)")
        print(f"      - ç›®æ ‡å‰ªæžçŽ‡ (Ratio): {self.prune_ratio * 100}%")
        print(f"    > [Surgery] å®Œæˆå‰ªæžã€‚")

        # SwanLab è®°å½•æ•æ„Ÿåº¦æŒ‡æ ‡
        try:
            swanlab.log(
                {
                    "round": self.round_index,
                    "Server/Saliency/mean": float(global_avg_saliency),
                    "Server/Saliency/pruned_pct": float(prune_percentage),
                    "Server/Saliency/total_lora_params": int(total_lora_params),
                }
            )
        except Exception:
            pass

        # =======================================================
        # 6. å¯¼å‡ºå¤„ç†åŽçš„å‚æ•° (Export)
        # =======================================================
        # ä½¿ç”¨å®˜æ–¹ API å¯¼å‡ºï¼Œç¡®ä¿ Key æ ¼å¼æ ‡å‡†ï¼Œæ–¹ä¾¿åŽç»­èšåˆ
        final_dict = get_peft_model_state_dict(self.model)
        
        # è½¬å›ž CPU èŠ‚çœæ˜¾å­˜
        final_dict = {k: v.cpu() for k, v in final_dict.items()}
            
        return final_dict
    def aggregate(self, client_state_dicts: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """
        å¯¹æ‰€æœ‰å®¢æˆ·ç«¯åšå‰ªæž + ç¼©æ”¾åŽï¼Œå†åšç®€å•å¹³å‡ (AvgMerge)ã€‚
        """
        n_clients = len(client_state_dicts)
        processed_models: List[Dict[str, torch.Tensor]] = []

        # ä¸º SwanLab å¢žåŠ  round ç»´åº¦ï¼ˆä¸Ž FedServer çš„é€šä¿¡è½®æ¬¡å¯¹é½ï¼‰
        self.round_index += 1

        print(f"Starting Sensitivity-Based Pruning (Ratio={self.prune_ratio})...")

        for idx, client_dict in enumerate(client_state_dicts):
            print(f"  > Processing Client {idx} ...")
            processed = self.compute_saliency_and_prune(client_dict, client_index=idx)
            processed_models.append(processed)

        print("  > Aggregating processed models...")
        avg_state_dict: Dict[str, torch.Tensor] = copy.deepcopy(processed_models[0])

        for key, value in avg_state_dict.items():
            if not isinstance(value, torch.Tensor):
                continue

            summed = processed_models[0][key].clone()
            for i in range(1, n_clients):
                summed += processed_models[i][key]
            avg_state_dict[key] = summed / float(n_clients)

        return avg_state_dict
